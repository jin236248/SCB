{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6006a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Using cached huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-1.25.0-cp311-cp311-win_amd64.whl (15.0 MB)\n",
      "                                              0.0/15.0 MB ? eta -:--:--\n",
      "                                              0.0/15.0 MB ? eta -:--:--\n",
      "                                              0.1/15.0 MB 1.6 MB/s eta 0:00:10\n",
      "                                              0.4/15.0 MB 3.3 MB/s eta 0:00:05\n",
      "     -                                        0.7/15.0 MB 4.7 MB/s eta 0:00:04\n",
      "     --                                       1.1/15.0 MB 5.8 MB/s eta 0:00:03\n",
      "     ----                                     1.6/15.0 MB 6.8 MB/s eta 0:00:02\n",
      "     ------                                   2.3/15.0 MB 8.2 MB/s eta 0:00:02\n",
      "     --------                                 3.1/15.0 MB 10.0 MB/s eta 0:00:02\n",
      "     ----------                               4.0/15.0 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------                             4.8/15.0 MB 12.4 MB/s eta 0:00:01\n",
      "     ----------------                         6.1/15.0 MB 13.9 MB/s eta 0:00:01\n",
      "     -------------------                      7.5/15.0 MB 15.9 MB/s eta 0:00:01\n",
      "     -----------------------                  9.0/15.0 MB 17.9 MB/s eta 0:00:01\n",
      "     ----------------------------            11.0/15.0 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       13.1/15.0 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.0/15.0 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.0/15.0 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.0/15.0 MB 34.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0-cp311-cp311-win_amd64.whl (143 kB)\n",
      "                                              0.0/143.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 143.2/143.2 kB 8.3 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "                                              0.0/268.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 268.0/268.0 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "                                              0.0/62.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.6/62.6 kB ? eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "                                              0.0/3.5 MB ? eta -:--:--\n",
      "     -----------------------------            2.6/3.5 MB 83.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 73.6 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.1-cp311-cp311-win_amd64.whl (263 kB)\n",
      "                                              0.0/263.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 263.7/263.7 kB 15.8 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.14.1->transformers)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "                                              0.0/163.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 163.8/163.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.1.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "                                              0.0/96.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 96.7/96.7 kB ? eta 0:00:00\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "                                              0.0/123.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.6/123.6 kB 7.1 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "                                              0.0/157.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 157.0/157.0 kB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, safetensors, urllib3, tqdm, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.15.1 idna-3.4 numpy-1.25.0 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.30.2 urllib3-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0305640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (1.25.0)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.1-cp311-cp311-win_amd64.whl (21.5 MB)\n",
      "                                              0.0/21.5 MB ? eta -:--:--\n",
      "                                              0.0/21.5 MB ? eta -:--:--\n",
      "                                              0.1/21.5 MB 1.2 MB/s eta 0:00:19\n",
      "                                              0.4/21.5 MB 3.3 MB/s eta 0:00:07\n",
      "     --                                       1.1/21.5 MB 7.3 MB/s eta 0:00:03\n",
      "     -----                                    2.7/21.5 MB 14.4 MB/s eta 0:00:02\n",
      "     ---------                                5.2/21.5 MB 22.2 MB/s eta 0:00:01\n",
      "     ---------------                          8.2/21.5 MB 30.8 MB/s eta 0:00:01\n",
      "     ---------------                          8.2/21.5 MB 30.8 MB/s eta 0:00:01\n",
      "     ---------------                          8.3/21.5 MB 24.1 MB/s eta 0:00:01\n",
      "     ---------------                          8.4/21.5 MB 22.5 MB/s eta 0:00:01\n",
      "     ----------------                         8.7/21.5 MB 20.6 MB/s eta 0:00:01\n",
      "     ----------------                         9.0/21.5 MB 19.2 MB/s eta 0:00:01\n",
      "     -----------------                        9.3/21.5 MB 18.0 MB/s eta 0:00:01\n",
      "     -----------------                        9.5/21.5 MB 17.4 MB/s eta 0:00:01\n",
      "     ------------------                       9.9/21.5 MB 16.7 MB/s eta 0:00:01\n",
      "     ------------------                      10.4/21.5 MB 18.7 MB/s eta 0:00:01\n",
      "     -------------------                     10.8/21.5 MB 18.2 MB/s eta 0:00:01\n",
      "     --------------------                    11.1/21.5 MB 17.7 MB/s eta 0:00:01\n",
      "     -------------------------               14.3/21.5 MB 18.2 MB/s eta 0:00:01\n",
      "     ---------------------------             15.2/21.5 MB 17.3 MB/s eta 0:00:01\n",
      "     ---------------------------------       18.2/21.5 MB 17.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   20.4/21.5 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  21.5/21.5 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  21.5/21.5 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  21.5/21.5 MB 59.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 21.5/21.5 MB 36.2 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.0.2-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "                                              0.0/10.6 MB ? eta -:--:--\n",
      "     ---------                                2.4/10.6 MB 51.9 MB/s eta 0:00:01\n",
      "     -----------------                        4.7/10.6 MB 59.6 MB/s eta 0:00:01\n",
      "     --------------------------               7.0/10.6 MB 63.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    10.0/10.6 MB 63.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.6/10.6 MB 50.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "                                              0.0/134.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 134.3/134.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.4-cp311-cp311-win_amd64.whl (317 kB)\n",
      "                                              0.0/317.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 317.2/317.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from datasets) (6.0)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "                                              0.0/61.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.2/61.2 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp311-cp311-win_amd64.whl (60 kB)\n",
      "                                              0.0/60.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.2/60.2 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp311-cp311-win_amd64.whl (32 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->datasets)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, multidict, frozenlist, dill, attrs, async-timeout, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 datasets-2.13.1 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 pandas-2.0.2 pyarrow-12.0.1 pytz-2023.3 tzdata-2023.3 xxhash-3.2.0 yarl-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bfecb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset multi_news (C:/Users/jin23/.cache/huggingface/datasets/multi_news/default/1.0.0/2f1f69a2bedc8ad1c5d8ae5148e4755ee7095f465c1c01ae8f85454342065a72)\n",
      "100%|██████████| 3/3 [00:00<00:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['document', 'summary']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"multi_news\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6a99fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (excerpt of 2000 characters, total length: 5353):\n",
      "LOS ANGELES (AP) — In her first interview since the NBA banned her estranged husband, Shelly Sterling says she will fight to keep her share of the Los Angeles Clippers and plans one day to divorce Donald Sterling. \n",
      " \n",
      " (Click Prev or Next to continue viewing images.) \n",
      " \n",
      " ADVERTISEMENT (Click Prev or Next to continue viewing images.) \n",
      " \n",
      " Los Angeles Clippers co-owner Shelly Sterling, below, watches the Clippers play the Oklahoma City Thunder along with her attorney, Pierce O'Donnell, in the first half of Game 3 of the Western Conference... (Associated Press) \n",
      " \n",
      " Shelly Sterling spoke to Barbara Walters, and ABC News posted a short story with excerpts from the conversation Sunday. \n",
      " \n",
      " NBA Commissioner Adam Silver has banned Donald Sterling for making racist comments and urged owners to force Sterling to sell the team. Silver added that no decisions had been made about the rest of Sterling's family. \n",
      " \n",
      " According to ABC's story, Shelly Sterling told Walters: \"I will fight that decision.\" \n",
      " \n",
      " Sterling also said that she \"eventually\" will divorce her husband, and that she hadn't yet done so due to financial considerations. ||||| Shelly Sterling said today that \"eventually, I am going to\" divorce her estranged husband, Donald Sterling, and if the NBA tries to force her to sell her half of the Los Angeles Clippers, she would \"absolutely\" fight to keep her stake in the team. \n",
      " \n",
      " \"I will fight that decision,\" she told ABC News' Barbara Walters today in an exclusive interview. \"To be honest with you, I'm wondering if a wife of one of the owners, and there's 30 owners, did something like that, said those racial slurs, would they oust the husband? Or would they leave the husband in?\" \n",
      " \n",
      " Sterling added that the Clippers franchise is her \"passion\" and \"legacy to my family.\" \n",
      " \n",
      " \"I've been with the team for 33 years, through the good times and the bad times,\" she added. \n",
      " \n",
      " These comments come nearly two weeks after NBA Commissioner Adam Silver announced a lifetime ban and a $2.5 \n",
      "\n",
      "Summary (length: 501):\n",
      "– Shelly Sterling plans \"eventually\" to divorce her estranged husband Donald, she tells Barbara Walters at ABC News. As for her stake in the Los Angeles Clippers, she plans to keep it, the AP notes. Sterling says she would \"absolutely\" fight any NBA decision to force her to sell the team. The team is her \"legacy\" to her family, she says. \"To be honest with you, I'm wondering if a wife of one of the owners … said those racial slurs, would they oust the husband? Or would they leave the husband in?\"\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"Document (excerpt of 2000 characters, total length: {len(sample[\"document\"])}):\"\"\")\n",
    "print(sample[\"document\"][:2000])\n",
    "print(f'\\nSummary (length: {len(sample[\"summary\"])}):')\n",
    "print(sample[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50a181aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (2.0.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "                                              0.0/1.2 MB ? eta -:--:--\n",
      "     --                                       0.1/1.2 MB 919.0 kB/s eta 0:00:02\n",
      "     -------------                            0.4/1.2 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.2/1.2 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.2/1.2 MB 6.9 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.2-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.1/2.1 MB 46.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torchvision) (1.25.0)\n",
      "Requirement already satisfied: requests in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading Pillow-9.5.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "                                              0.0/2.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.5/2.5 MB 78.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->torchvision) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: pillow, torchvision, torchaudio\n",
      "Successfully installed pillow-9.5.0 torchaudio-2.0.2 torchvision-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8aa5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jin23\\.conda\\envs\\scb\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading pytorch_model.bin: 100%|██████████| 460M/460M [00:26<00:00, 17.6MB/s] \n",
      "c:\\Users\\jin23\\.conda\\envs\\scb\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jin23\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "model_ckpt = \"sshleifer/distilbart-cnn-6-6\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085bb5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\jin23\\.cache\\huggingface\\datasets\\multi_news\\default\\1.0.0\\2f1f69a2bedc8ad1c5d8ae5148e4755ee7095f465c1c01ae8f85454342065a72\\cache-7eeb1f0d34650a49.arrow\n",
      "Map:   0%|          | 0/5622 [00:00<?, ? examples/s]c:\\Users\\jin23\\.conda\\envs\\scb\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Loading cached processed dataset at C:\\Users\\jin23\\.cache\\huggingface\\datasets\\multi_news\\default\\1.0.0\\2f1f69a2bedc8ad1c5d8ae5148e4755ee7095f465c1c01ae8f85454342065a72\\cache-3a340bcbf3a3f4d2.arrow\n"
     ]
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"document\"], max_length=1024, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length=256, truncation=True)\n",
    "        \n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"], \n",
    "           \"attention_mask\": input_encodings[\"attention_mask\"], \n",
    "           \"labels\": target_encodings[\"input_ids\"]}\n",
    "dataset_pt = dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8747f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"test\"][1][\"document\"]\n",
    "reference = dataset[\"test\"][1][\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b56359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " UPDATE: 4/19/2001 Read Richard Metzger: How I, a married, middle-aged man, became an accidental spokesperson for gay rights overnight on Boing Boing \n",
      " \n",
      " It’s time to clarify a few details about the controversial “Hey Facebook what’s SO wrong with a pic of two men kissing?” story, as it now beginning to be reported in the mainstream media, and not always correctly. \n",
      " \n",
      " First of all, with regards to the picture: \n",
      " \n",
      " The photo which was used to illustrate my first post about the John Snow Kiss-In is a promotional still from the British soap opera “Eastenders.” It features one of the main characters from the show (Christian Clarke, played by the actor John Partridge- left) and someone else who I don’t know. I am not a regular viewer so I can’t say if the man on the right is an extra or an actual character. \n",
      " \n",
      " This picture has itself caused scandal in the UK, as it was a gay kiss that was broadcast before the watershed, and as such led to a number of complaints to the BBC. However, since this episode aired (October 2008) Christian now has a boyfriend and a few more gay kisses have taken place. \n",
      " \n",
      " In relation to the John Snow Kiss-In event, I used this particular photo because I considered it to be quite mild (no groping, no tongues). The photos I had considered using before I chose that one are much more racy. Oh the irony! \n",
      " \n",
      " Secondly, the removal of the Facebook John Snow Kiss-In event: \n",
      " \n",
      " It turns out that the Facebook event for the John Snow Kiss-In was not blocked by Facebook, but made private by the creator of the event itself. Paul Shetler, the organizer, left this comment on the previous thread: \n",
      " \n",
      " “Hey I just saw this. Before it goes too far, I just want people to know that FB have NOT removed the kiss-in event page; it’s still there, but _I made the event private after the event_ was over and only visible to those who had been invited as there were starting to be trolls posting abusive nonsense on it.” \n",
      " \n",
      " Thanks for clearing that up, Paul. Now if Facebook will only reply to Richard’s query about why they removed my original post and photo when he put it up on his wall… \n",
      " \n",
      " It has been erroneously reported in the media that our own Richard Metzger (who lives in Los Angeles) organized the London “Kiss-In” event, which is untrue, and also unfair to Paul Shetler and the actual organizers. Also, Richard did not state in his post that Facebook HAD taken the event page down, he just questioned IF this was the case and IF there was a connection with MY post about the event being removed from his own wall. This seems to have confused some people. \n",
      " \n",
      " Here is a report on the John Snow Kiss-In from the Guardian, featuring an interview with Mr Shetler: \n",
      " \n",
      " ||||| || News || \n",
      " \n",
      " Page 1 of 1 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " UPDATED: A photo of two men kissing that was posted on a Facebook page protesting a London pub’s decision to eject a same-sex couple for kissing has been removed by the social networking site, an error, according to a rep for the company. \n",
      " \n",
      " \n",
      " \n",
      " \"The photo in question does not violate our Statement of Rights and Responsibilities and was removed in error,\" the statement, obtained by America Blog, says. We apologize for the inconvenience\" \n",
      " \n",
      " \n",
      " \n",
      " The Dangerous Minds Facebook page was set up to promote a “gay kiss-in” demonstration in London to protest the pub. The page used a photo of two men kissing to promote the event. \n",
      " \n",
      " According to NYULocal.com, the photo was quickly removed and the following e-mail was sent to administrators of the Facebook page: “Shares that contain nudity, or any kind of graphic or sexually suggestive content, are not permitted on Facebook.” \n",
      " \n",
      " \n",
      " \n",
      " The decision to remove the photo has prompted scores of people to post their own pictures of same-sex couples kissing in protest — dozens in the last few hours alone.\n",
      "\n",
      "Reference Summary:\n",
      "– It turns out Facebook is only guilty of about half of what it’s been accused of in the gay kiss incident. The social networking site apologized yesterday for taking down an image used to promote a “kiss-in” event in London. “The photo in question does not violate our Statement of Rights and Responsibilities, and was removed in error,” the site said in a statement, according to the Advocate. But Facebook did not, as has been reported in several places, take down the kiss-in event itself. Here’s what happened: The photo Facebook took down was posted by the Dangerous Minds blog to promote the event. In its initial write-up about the incident, the blog observed that the page organizing the protest had been taken down. But it was actually the organizer himself who \"removed\" the event, Dangerous Minds clarified. Organizer Paul Shetler explains that he decided to switch it from a public event to a private one, as \"there were starting to be trolls posting abusive nonsense on it.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Document:\")\n",
    "print(sample_text)\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421d5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir='bart-multi-news', num_train_epochs=1, warmup_steps=500,                                  per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=False, \n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, \n",
    "gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d77434f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mmodel, args\u001b[39m=\u001b[39mtraining_args, tokenizer\u001b[39m=\u001b[39mtokenizer,                  data_collator\u001b[39m=\u001b[39mseq2seq_data_collator,                  train_dataset\u001b[39m=\u001b[39mdataset_pt[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],                  eval_dataset\u001b[39m=\u001b[39mdataset_pt[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer,                  data_collator=seq2seq_data_collator,                  train_dataset=dataset_pt[\"train\"],                  eval_dataset=dataset_pt[\"validation\"])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c749d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(sample_text, max_length=1024, truncation=True, padding='max_length', return_tensors='pt').to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a75d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = model.generate(input_ids=input_ids['input_ids'], attention_mask=input_ids['attention_mask'],max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4b625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d56c5008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reference Summary:\n",
      "– It turns out Facebook is only guilty of about half of what it’s been accused of in the gay kiss incident. The social networking site apologized yesterday for taking down an image used to promote a “kiss-in” event in London. “The photo in question does not violate our Statement of Rights and Responsibilities, and was removed in error,” the site said in a statement, according to the Advocate. But Facebook did not, as has been reported in several places, take down the kiss-in event itself. Here’s what happened: The photo Facebook took down was posted by the Dangerous Minds blog to promote the event. In its initial write-up about the incident, the blog observed that the page organizing the protest had been taken down. But it was actually the organizer himself who \"removed\" the event, Dangerous Minds clarified. Organizer Paul Shetler explains that he decided to switch it from a public event to a private one, as \"there were starting to be trolls posting abusive nonsense on it.\"\n",
      "\n",
      "Model Summary:\n",
      " The photo was used to illustrate my first post about the John Snow Kiss-In is a promotional still from the British soap opera ‘Eastenders’ The photo has itself caused scandal in the UK, as it was a gay kiss that was broadcast before the watershed, and as such led to a number of complaints to the BBC\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "print(\"\\nModel Summary:\")\n",
    "print(decoded_summaries[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
