{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "                                              0.0/563.8 kB ? eta -:--:--\n",
      "     -                                     20.5/563.8 kB 640.0 kB/s eta 0:00:01\n",
      "     ----                                  71.7/563.8 kB 975.2 kB/s eta 0:00:01\n",
      "     --------------------------             399.4/563.8 kB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 563.8/563.8 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers==2.8.0) (1.25.0)\n",
      "Collecting tokenizers==0.5.2 (from transformers==2.8.0)\n",
      "  Downloading tokenizers-0.5.2.tar.gz (64 kB)\n",
      "                                              0.0/64.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.6/64.6 kB ? eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting boto3 (from transformers==2.8.0)\n",
      "  Downloading boto3-1.26.163-py3-none-any.whl (135 kB)\n",
      "                                              0.0/135.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 135.9/135.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers==2.8.0) (3.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers==2.8.0) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from transformers==2.8.0) (2023.6.3)\n",
      "Collecting sentencepiece (from transformers==2.8.0)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "                                              0.0/977.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 977.5/977.5 kB 31.2 MB/s eta 0:00:00\n",
      "Collecting sacremoses (from transformers==2.8.0)\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "                                              0.0/880.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 880.6/880.6 kB 54.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from tqdm>=4.27->transformers==2.8.0) (0.4.6)\n",
      "Collecting botocore<1.30.0,>=1.29.163 (from boto3->transformers==2.8.0)\n",
      "  Downloading botocore-1.29.163-py3-none-any.whl (11.0 MB)\n",
      "                                              0.0/11.0 MB ? eta -:--:--\n",
      "     ----------                               2.8/11.0 MB 91.5 MB/s eta 0:00:01\n",
      "     ----------------------------            8.0/11.0 MB 102.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.0/11.0 MB 93.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.0/11.0 MB 72.5 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers==2.8.0)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->transformers==2.8.0)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "                                              0.0/79.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.8/79.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->transformers==2.8.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->transformers==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->transformers==2.8.0) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from requests->transformers==2.8.0) (2023.5.7)\n",
      "Requirement already satisfied: six in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from sacremoses->transformers==2.8.0) (1.16.0)\n",
      "Collecting click (from sacremoses->transformers==2.8.0)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib (from sacremoses->transformers==2.8.0)\n",
      "  Downloading joblib-1.3.0-py3-none-any.whl (301 kB)\n",
      "                                              0.0/301.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 301.9/301.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jin23\\.conda\\envs\\scb\\lib\\site-packages (from botocore<1.30.0,>=1.29.163->boto3->transformers==2.8.0) (2.8.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==2.8.0)\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "                                              0.0/143.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 143.1/143.1 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: tokenizers, sacremoses\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895252 sha256=10845517ef05c890d46a8b6ff8021ef67fe4117754ce67aac7e167f9555ea6e9\n",
      "  Stored in directory: c:\\users\\jin23\\appdata\\local\\pip\\cache\\wheels\\11\\75\\c6\\a82d827a00df823caf211262900d2c024f5b3a775b82b45230\n",
      "Successfully built sacremoses\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [46 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 2.0.0, 2.0.1)\n",
      "ERROR: No matching distribution found for torch==1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.8.0\n",
    "!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-win_amd64.whl (977 kB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jin23\\.conda\\envs\\scb\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 1.09MB/s]\n",
      "c:\\Users\\jin23\\.conda\\envs\\scb\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jin23\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.32k/2.32k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"\"\"\n",
    "The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.\n",
    "The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.\n",
    "At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\n",
    "\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"\n",
    "The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed: \n",
      " The US has \"passed the peak\" on new coronavirus cases, President Donald Trump said and predicted that some states would reopen this month.The US has over 637,000 confirmed Covid-19 cases and over 30,826 deaths, the highest for any country in the world.At the daily White House coronavirus briefing on Wednesday, Trump said new guidelines to reopen the country would be announced on Thursday after he speaks to governors.\"We'll be the comeback kids, all of us,\" he said. \"We want to get our country back.\"The Trump administration has previously fixed May 1 as a possible date to reopen the world's largest economy, but the president said some states may be able to return to normalcy earlier than that.\n"
     ]
    }
   ],
   "source": [
    "preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
    "t5_prepared_Text = \"summarize: \"+preprocess_text\n",
    "print (\"original text preprocessed: \\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summarized text: \n",
      " the president predicts some states will reopen this month. the u.s. has over 637,000 confirmed cases and over 30,826 deaths, the highest for any country in the world. \"we want to get our country back,\" he says.\n"
     ]
    }
   ],
   "source": [
    "summary_ids = model.generate(tokenized_text,\n",
    "                                    num_beams=4,\n",
    "                                    no_repeat_ngram_size=2,\n",
    "                                    min_length=30,\n",
    "                                    max_length=100,\n",
    "                                    early_stopping=True)\n",
    "\n",
    "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (\"\\n\\nSummarized text: \\n\",output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
